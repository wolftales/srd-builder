# v0.15.1 Spell Refactoring Plan

## Investigation Results

**Date:** 2025-12-21
**Baseline:** v0.15.0

### Key Findings

1. **Font Metadata** (pages 117-119):
   - 6 font families detected
   - Spell names: `GillSans-SemiBold` at 12.0pt
   - Body text: `Cambria` at 9.8pt
   - Emphasis/Italics: `Cambria-Italic` at 9.8pt
   - Field labels: `Cambria-Bold` at 9.8pt (Casting Time, Range, etc.)
   - Sub-headers: `Cambria-BoldItalic` at 9.8pt

2. **Boundary Contamination**:
   - ✅ **CLEAN** - No spell boundary issues detected
   - All 319 spells properly segmented
   - Unlike conditions (which had contamination), spell extraction boundaries are correct

3. **Paragraph Spacing**:
   - Negative gaps (-4.92px average) indicate overlapping bounding boxes
   - PyMuPDF line height calculation inconsistent
   - Threshold approach won't work reliably for spell paragraphs
   - **Recommendation:** Use explicit `\n\n` detection in text instead

4. **Column Layout**:
   - Two-column layout confirmed
   - Page width: 612pt
   - Left column: ~160pt average X position
   - Right column: starts after midpoint (306pt)

### Technical Debt Analysis

From code review of [parse_spells.py](../../src/srd_builder/parse/parse_spells.py):

**Complexity Violations:**
- `parse_spell_records()`: C901 complexity 11 (threshold: 10)
- Main issues:
  1. Multi-page spell handling uses hacky SRD marker splitting
  2. Regex lookahead patterns fragile (`(?= next spell name)`)
  3. Single text blob storage limits quality
  4. No font metadata captured for semantic parsing

**Current Approach:**
```python
# Multi-page handling: split on SRD markers
parts = text.split("System Reference Document 5.1")

# Boundary detection: regex lookahead to next spell
pattern = rf"(.*?)(?={re.escape(next_name)}\s+\d+(st|nd|rd|th)"
```

**Problems:**
1. Assumes SRD markers always present (brittle)
2. Regex lookahead fails with similar spell names
3. No paragraph structure preservation
4. Font metadata ignored (can't distinguish headers from body)

## Refactoring Plan

### Phase 1: Extract Font Metadata (v0.15.1)

**Goal:** Capture font information during extraction for semantic analysis.

**Changes to [extract_spells.py](../../src/srd_builder/extract/extract_spells.py):**

```python
# Add to each text block:
{
    "text": span_text,
    "font": {
        "name": span["font"],
        "size": round(span["size"], 1),
        "is_bold": "Bold" in span["font"],
        "is_italic": "Italic" in span["font"],
    },
    "position": {
        "x": span["bbox"][0],
        "y": span["bbox"][1],
    }
}
```

**Benefits:**
- Enable semantic parsing (spell names vs headers vs body)
- Support future rules.json extraction (needs font-tier hierarchy)
- Better paragraph detection (font changes indicate structure)

**Test Cases:**
- Verify font metadata extracted for all spell blocks
- Validate font categorization (name/header/body)
- Check position accuracy

### Phase 2: Reduce Complexity (v0.15.1)

**Goal:** Refactor `parse_spell_records()` from C901:11 → C901:<10.

**Approach:**
1. Extract multi-page handling into separate function:
   ```python
   def _merge_multipage_spell(current: dict, next_spell: dict | None) -> dict:
       """Handle spells split across pages."""
   ```

2. Simplify regex patterns (remove lookahead):
   ```python
   # Use boundary markers from extraction instead
   def _detect_spell_boundary(text_blocks: list[dict]) -> list[int]:
       """Find spell boundaries using font size (12pt = new spell)."""
   ```

3. Extract field parsing into helper:
   ```python
   def _extract_spell_fields(text: str) -> dict[str, str]:
       """Parse Casting Time, Range, Components, etc."""
   ```

**Complexity Targets:**
- `parse_spell_records()`: 6 branches (down from 11)
- `_merge_multipage_spell()`: 4 branches (new)
- `_extract_spell_fields()`: 5 branches (new)

### Phase 3: Paragraph Segmentation (v0.15.1)

**Goal:** Store descriptions as paragraph arrays instead of text blobs.

**Investigation Finding:** Negative spacing values indicate paragraph detection via Y-position gaps won't work reliably.

**Alternative Approach:** Text-based paragraph detection:

```python
def _segment_paragraphs(text: str) -> list[str]:
    """Split spell description into paragraphs.

    Uses explicit double-newline detection instead of Y-position gaps.
    """
    # Split on double newlines (already in extracted text)
    paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]
    return paragraphs
```

**Schema Update:**

```python
# Before:
{"description": "Long text blob..."}

# After:
{"description": ["Para 1...", "Para 2...", "Para 3..."]}
```

**Benefits:**
- Easier to parse individual paragraphs
- Better for downstream consumers (API, UIs)
- Matches conditions/diseases pattern (they use arrays)

### Phase 4: Validation & Testing

**Test Plan:**

1. **Golden Fixtures:**
   - Update [tests/fixtures/spells/raw/](../../tests/fixtures/spells/raw/) with font metadata
   - Update [tests/fixtures/spells/normalized/](../../tests/fixtures/spells/normalized/) with paragraph arrays
   - Regenerate using `scripts/bump_version.py --fixtures-only`

2. **Complexity Check:**
   ```bash
   ruff check src/srd_builder/parse/parse_spells.py | grep C901
   # Should be empty (no violations)
   ```

3. **Full Test Suite:**
   ```bash
   pytest tests/test_parse_spells.py -v
   pytest tests/test_golden_spells.py -v
   pytest -q  # All 173 tests
   ```

4. **Build Verification:**
   ```bash
   make build
   # Verify all 319 spells parse correctly
   # Compare output with baseline (should match except schema changes)
   ```

**Success Criteria:**
- ✅ All C901 violations resolved
- ✅ Font metadata captured in raw extraction
- ✅ Descriptions stored as paragraph arrays
- ✅ All 319 spells parse correctly
- ✅ All tests passing
- ✅ No data loss vs baseline

## Timeline

**Estimated Effort:** 3-4 hours
- Phase 1: 1 hour (font metadata extraction)
- Phase 2: 1 hour (complexity reduction)
- Phase 3: 1 hour (paragraph arrays)
- Phase 4: 1 hour (testing & validation)

**Dependencies:**
- None (self-contained refactoring)

**Follow-up Work (v0.16.0):**
- Rules.json hierarchical extraction (will reuse font metadata patterns)
- Spell class validation (cross-reference with class lists)

## Notes

### Why Spells Before Rules.json?

1. **Lower Risk:** Spells are well-tested (319 golden fixtures)
2. **Prove Patterns:** Font metadata approach validates before complex hierarchical extraction
3. **Shared Infrastructure:** Both need semantic parsing via font tiers
4. **Incremental Value:** Improves spell quality immediately (paragraph arrays)

### Lessons from Conditions v0.10.0

**What Worked:**
- Font-tier approach for semantic headers
- Column-aware extraction
- Paragraph arrays for structure

**What to Avoid:**
- Y-position gap detection (unreliable with negative gaps)
- Over-reliance on regex patterns (font metadata cleaner)

**Applicability to Spells:**
- ✅ Font-tier approach: spell names (12pt) vs body (9.8pt)
- ✅ Column-aware extraction: already working
- ✅ Paragraph arrays: text-based detection instead of Y-gaps
- ❌ Case sensitivity: not needed (spell names well-formatted)

## References

- Investigation results: [spell_investigation_results.json](../../scripts/spell_investigation_results.json)
- Current implementation: [parse_spells.py](../../src/srd_builder/parse/parse_spells.py), [extract_spells.py](../../src/srd_builder/extract/extract_spells.py)
- Conditions refactoring (v0.10.0): [HANDOFF_v0.13.0.md](../HANDOFF_v0.13.0.md)
- Complexity guidelines: [AGENTS.md](../../AGENTS.md)
