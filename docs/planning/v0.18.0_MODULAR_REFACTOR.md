# v0.18.0 Plan â€” Modular Refactor & Testing

**Status:** ðŸ“‹ Planning
**Started:** December 22, 2024
**Target:** Before v1.0.0
**Milestone:** Standardize all datasets to modular pattern

---

## Objectives

Refactor all remaining monolithic parsers to use the modular Extract â†’ Parse â†’ Postprocess pattern established in v0.15.0+ (monsters, spells, equipment, magic_items, rules).

**Benefits:**
- Consistent architecture across all 12 datasets
- Better testability (parse and postprocess tested independently)
- Easier maintenance and debugging
- Clear separation of concerns (parsing vs normalization)
- Enables golden test coverage for all datasets

---

## Datasets to Refactor

**7 datasets currently using monolithic pattern:**

1. **poisons** - Poison items from table (14 items)
2. **diseases** - Disease descriptions (3 items: Cackle Fever, Sewer Plague, Sight Rot)
3. **conditions** - Game conditions (15 items: Blinded, Poisoned, etc.)
4. **lineages** - Character ancestry (13 items: Dwarf, Elf, Human, etc.)
5. **features** - Class/lineage abilities (246 items: 154 class + 92 lineage)
6. **tables** - Reference tables (37 items: 12 class progression + 25 reference)
7. **classes** - Character classes (12 items: Fighter, Wizard, Rogue, etc.)

**Order:** Simplest â†’ Most Complex

---

## Current State Analysis

### Existing Modular Pattern (5 datasets)
- âœ… **monsters** - parse_monsters.py + postprocess/monsters.py + test_golden_monsters.py
- âœ… **equipment** - parse_equipment.py + postprocess/equipment.py + test_golden_equipment.py
- âœ… **spells** - parse_spells.py + postprocess/spells.py + test_golden_spells.py
- âœ… **magic_items** - parse_magic_items.py + postprocess/magic_items.py + test_golden_magic_items.py
- âœ… **rules** - parse_rules.py + postprocess/rules.py + test_golden_rules.py

### Monolithic Pattern (7 datasets)
- âŒ **poisons** - parse_poisons.py (monolithic)
- âŒ **diseases** - parse_diseases.py (monolithic)
- âŒ **conditions** - parse_conditions.py (monolithic)
- âŒ **lineages** - parse_lineages.py (monolithic)
- âŒ **features** - parse_features.py (monolithic)
- âŒ **tables** - parse_tables.py (monolithic)
- âŒ **classes** - parse_classes.py (monolithic)

---

## Refactor Pattern

For each dataset, follow this workflow:

### 1. Analyze Current Parser
- Read existing parse_X.py
- Identify parsing logic (structure extraction)
- Identify normalization logic (IDs, text cleanup, field standardization)
- Document any edge cases or special handling

### 2. Split into Parse + Postprocess

**Parse Module (parse/parse_X.py):**
- Extract structure from raw data
- Return unnormalized dicts
- No ID generation, no text cleanup
- Pure parsing logic only

**Postprocess Module (postprocess/X.py):**
- Create clean_X_record() function
- Use normalize_id() from postprocess/ids.py
- Use clean_text() and polish_text() from postprocess/text.py
- Generate id and simple_name fields
- Clean all text fields
- Return normalized dict

### 3. Create Golden Test

**Fixtures:**
- tests/fixtures/srd_5_1/raw/X.json - Raw data (5-10 representative samples)
- tests/fixtures/srd_5_1/normalized/X.json - Expected output after postprocess

**Test File (tests/test_golden_X.py):**
```python
from pathlib import Path
import json
from srd_builder.parse.parse_X import parse_X_records
from srd_builder.postprocess import clean_X_record
from srd_builder.utils.metadata import meta_block, read_schema_version

def test_X_dataset_matches_normalized_fixture() -> None:
    raw_path = Path("tests/fixtures/srd_5_1/raw/X.json")
    expected_path = Path("tests/fixtures/srd_5_1/normalized/X.json")

    X_raw = json.loads(raw_path.read_text(encoding="utf-8"))
    parsed = parse_X_records(X_raw)
    processed = [clean_X_record(item) for item in parsed]

    document = {"_meta": meta_block("srd_5_1", read_schema_version("X")), "items": processed}

    rendered = json.dumps(document, indent=2, ensure_ascii=False) + "\n"
    expected = expected_path.read_text(encoding="utf-8")
    assert rendered == expected
```

### 4. Update build.py

Replace:
```python
parsed_X = parse_X()  # Monolithic - returns normalized
```

With:
```python
from .postprocess import clean_X_record

# In build():
X_raw = _load_raw_X()
parsed_X = parse_X_records(X_raw)  # Returns unnormalized
cleaned_X = [clean_X_record(item) for item in parsed_X]
```

### 5. Verify Output

Compare v0.17.0 baseline (tagged) with refactored output:
```bash
# Save v0.17.0 baseline
git checkout v0.17.0
make bundle
cp dist/srd_5_1/X.json /tmp/X_baseline.json

# Build with refactor
git checkout main
make bundle
diff <(jq -S . /tmp/X_baseline.json) <(jq -S . dist/srd_5_1/X.json)
```

**Acceptable differences:**
- Intentional improvements (better text cleanup, more accurate IDs)
- Fixed bugs

**Unacceptable differences:**
- Missing fields
- Incorrect data
- Broken structure

### 6. Export from postprocess/__init__.py

Add to postprocess/__init__.py:
```python
from .X import clean_X_record

__all__ = [
    # ... existing exports ...
    "clean_X_record",
]
```

---

## Implementation Schedule

### Phase 1: Simple Datasets (1-2 days)
**Datasets:** poisons, diseases, conditions, lineages

**Characteristics:**
- Simple structure (mostly text + metadata)
- Few fields
- Minimal edge cases
- Good warmup for pattern

**Deliverable per dataset:**
- Split parse â†’ parse + postprocess
- Golden test with 5-10 samples
- build.py updated
- All tests passing
- Output verified against v0.17.0 baseline

### Phase 2: Complex Datasets (2-3 days)
**Datasets:** features, tables, classes

**Characteristics:**
- Nested structures
- Multiple record types
- Complex relationships
- More edge cases

**Deliverable per dataset:**
- Split parse â†’ parse + postprocess
- Golden test with comprehensive samples
- build.py updated
- All tests passing
- Output verified against v0.17.0 baseline

### Phase 3: Validation & Polish (1 day)
- Run full test suite (all 183+ tests)
- Verify all 12 datasets build correctly
- Check bundle completeness
- Update documentation (ARCHITECTURE.md)
- Final verification against v0.17.0 baseline

---

## Success Criteria

### Code Quality
- âœ… All 7 datasets refactored to modular pattern
- âœ… Zero code duplication (shared utilities used)
- âœ… Consistent naming conventions
- âœ… Clean separation: parse (structure) vs postprocess (normalization)

### Testing
- âœ… Golden tests for all 12 datasets (7 new + 5 existing)
- âœ… All tests passing (183+ tests)
- âœ… No regressions from v0.17.0

### Output Validation
- âœ… All dataset outputs match v0.17.0 baseline (or documented improvements)
- âœ… Schema compliance verified
- âœ… Bundle builds successfully

### Documentation
- âœ… ARCHITECTURE.md updated with modular pattern confirmation
- âœ… Release notes complete
- âœ… All datasets follow same documented pattern

---

## Risks & Mitigations

### Risk: Breaking Output Format
**Mitigation:** Golden tests + v0.17.0 baseline comparison for every dataset

### Risk: Undiscovered Edge Cases
**Mitigation:** Comprehensive fixture selection (edge cases + common cases)

### Risk: Refactor Takes Too Long
**Mitigation:** Incremental approach - commit after each dataset, can pause/resume

### Risk: Tests Don't Catch Issues
**Mitigation:** Run full build + smoke tests after each refactor

---

## Architectural Notes

### Module Boundaries (from ARCHITECTURE.md)

**Extract (src/srd_builder/extract/):**
- PDF â†’ Raw JSON (text blocks, font metadata, coordinates)
- No parsing, no normalization
- Pure extraction

**Parse (src/srd_builder/parse/):**
- Raw JSON â†’ Structured dicts (unnormalized)
- Identifies entities, builds hierarchy
- NO id generation, NO text cleanup
- Returns dicts with original text

**Postprocess (src/srd_builder/postprocess/):**
- Structured dicts â†’ Normalized records
- clean_X_record() function per dataset
- Uses shared utilities: normalize_id(), clean_text(), polish_text()
- Generates id and simple_name
- Cleans all text fields

**Build (src/srd_builder/build.py):**
- Orchestrates pipeline
- I/O operations only
- Calls extract â†’ parse â†’ postprocess â†’ index â†’ write

### Shared Utilities

**postprocess/ids.py:**
- normalize_id(text, namespace) - Generate consistent IDs

**postprocess/text.py:**
- clean_text(text) - Remove control characters (\r, \n)
- polish_text(text) - Format quotes, spaces, etc.

**utils/metadata.py:**
- meta_block(ruleset, schema_version) - Generate _meta wrapper
- read_schema_version(schema_name) - Read version from schema file

---

## Testing Strategy

### Golden Tests (Per Dataset)
- **Purpose:** Verify parse â†’ postprocess pipeline produces expected output
- **Coverage:** 5-10 representative samples per dataset
- **Pattern:** Modular (load raw â†’ parse â†’ postprocess â†’ compare)

### Integration Tests
- **test_build_pipeline.py:** Full build creates all expected files
- **test_dataset_completeness.py:** All datasets have expected counts
- **test_json_sanity.py:** JSON files are valid and well-formed

### Smoke Tests
- **scripts/smoke.sh:** Quick validation of bundle structure
- **scripts/verify_build.sh:** Comprehensive bundle verification

---

## Release Notes Template

For v0.18.0 release notes:

```markdown
## v0.18.0 - Modular Refactor & Testing (YYYY-MM-DD)

### Architectural Improvements
- Refactored 7 datasets to modular Extract â†’ Parse â†’ Postprocess pattern
- Standardized all 12 datasets to consistent architecture
- Zero code duplication across datasets

### Testing Enhancements
- Added golden tests for 7 datasets (poisons, diseases, conditions, lineages, features, tables, classes)
- 12/12 datasets now have golden test coverage
- All 183+ tests passing

### Datasets Refactored
1. **poisons** - Split parse_poisons.py â†’ parse + postprocess
2. **diseases** - Split parse_diseases.py â†’ parse + postprocess
3. **conditions** - Split parse_conditions.py â†’ parse + postprocess
4. **lineages** - Split parse_lineages.py â†’ parse + postprocess
5. **features** - Split parse_features.py â†’ parse + postprocess
6. **tables** - Split parse_tables.py â†’ parse + postprocess
7. **classes** - Split parse_classes.py â†’ parse + postprocess

### Verification
- All dataset outputs verified against v0.17.0 baseline
- No regressions
- Bundle builds successfully (2.8MB, 24 files)

### Breaking Changes
None - all changes are internal refactoring
```

---

## Related Documentation

- [ARCHITECTURE.md](../ARCHITECTURE.md) - Module boundaries, testing patterns
- [AGENTS.md](../../AGENTS.md) - Agent workflow and code quality guidelines
- [v0.17.0 Release Notes](../releases/v0.17.0_Release_Notes.md) - Previous milestone
- [ROADMAP.md](../ROADMAP.md) - Project roadmap

---

## Questions or Issues?

See [CONTRIBUTING.md](../../CONTRIBUTING.md) for development workflow.
